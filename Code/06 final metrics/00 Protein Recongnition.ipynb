{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "import time\n",
    "\n",
    "def done():\n",
    "    frequency = 1000  # Set Frequency To 2500 Hertz\n",
    "    duration = 500  # Set Duration To 1000 ms == 1 second\n",
    "    \n",
    "    \n",
    "    for i in range(5):\n",
    "        winsound.Beep(frequency, duration)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "import scispacy\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "def word2vec(x):\n",
    "    return nlp(x).vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing Proteins and Common English Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if os.path.isfile('dataset.npy') and os.path.isfile('proteinVectors.npy') and os.path.isfile('commonVectors.npy'):\n",
    "    dataset = np.load('dataset.npy', allow_pickle=True)\n",
    "    proteinVectors = np.load('proteinVectors.npy', allow_pickle=True)\n",
    "    commonVectors = np.load('commonVectors.npy', allow_pickle=True)\n",
    "else:\n",
    "    with open (\"protein_list.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "        lines=myfile.readlines()\n",
    "        myfile.close()\n",
    "\n",
    "    proteins = [l.rstrip() for l in lines]    \n",
    "    print(len(proteins),\"proteins imported\")\n",
    "    \n",
    "        \n",
    "    import nltk\n",
    "    nltk.download('words')\n",
    "    from nltk.corpus import words\n",
    "    word_list = words.words()\n",
    "    # prints 236736\n",
    "    \n",
    "    word_list = random.sample(word_list,len(proteins))   \n",
    "   \n",
    "    \n",
    "    words = [w for w in word_list]\n",
    "    print(len(words),\"words imported\")\n",
    "    \n",
    "    \n",
    "    for p in proteins:\n",
    "        if p in words:\n",
    "            words.remove(p)\n",
    "            \n",
    "    proteinVectors = [word2vec(p) for p in proteins]\n",
    "    \n",
    "    commonVectors = [word2vec(w) for w in words]\n",
    "\n",
    "    dataset = [(p,1) for p in proteinVectors]    \n",
    "    print(\"done with converting proteins to vectors\")\n",
    "    \n",
    "    dataset += [(w,0) for w in commonVectors]\n",
    "    print(\"done with converting words to vectors\")\n",
    "\n",
    "    np.save('dataset.npy', dataset)\n",
    "    np.save('proteinVectors.npy', proteinVectors)\n",
    "    np.save('commonVectors.npy', commonVectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192418\n",
      "192418\n",
      "38484\n",
      "(96,)\n",
      "0 1\n",
      "1 1\n",
      "2 0\n",
      "3 0\n",
      "4 1\n",
      "5 0\n",
      "6 0\n",
      "7 1\n",
      "8 1\n",
      "9 1\n",
      "10 0\n",
      "11 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#print(dataset[0])\n",
    "\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "#print(dataset[0])\n",
    "\n",
    "#dataset = random.sample(dataset,len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectors = [l[0] for l in dataset]\n",
    "labels = [l[1] for l in dataset]\n",
    "\n",
    "print(len(vectors))\n",
    "print(len(labels))\n",
    "print(round(len(labels)/5))\n",
    "ratio = round(len(labels)/5)\n",
    "\n",
    "print(vectors[0].shape)\n",
    "\n",
    "for i,k in enumerate(labels):\n",
    "    print(i,k)\n",
    "    if i>10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38484\n",
      "153934\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "testSet = np.array(vectors[:ratio])\n",
    "testLabel = labels[:ratio]\n",
    "\n",
    "trainSet = np.array(vectors[ratio:])\n",
    "trainLabel = labels[ratio:]\n",
    "\n",
    "print(len(testSet))\n",
    "print(len(trainSet))\n",
    "print(type(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "for t in trainSet:\n",
    "    print(t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "vectorSize = trainSet[0].shape[0]\n",
    "print(vectorSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "#from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Imports work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "mlp1 = joblib.load(\"proteinRecognitionNetwork.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153934/153934 [==============================] - 5s 31us/step\n",
      "\n",
      "38484/38484 [==============================] - 1s 28us/step\n",
      "0.9887744188308716 0.9882808327674866 0.9882807695341203 0.988279960267048 0.9882818790910916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "trainLoss, trainAccuracy = mlp1.evaluate(trainSet, trainLabel, verbose=1)\n",
    "#print('Training Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "testLoss, testAccuracy = mlp1.evaluate(testSet, testLabel, verbose=1)\n",
    "#print('Test Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = np.round(mlp1.predict(testSet))\n",
    "\n",
    "\n",
    "f1 = f1_score(testLabel, predictions, average=\"macro\")\n",
    "precision = precision_score(testLabel, predictions, average=\"macro\")\n",
    "recall = recall_score(testLabel, predictions, average=\"macro\")\n",
    "\n",
    "\n",
    "\n",
    "print(trainAccuracy,testAccuracy,f1,precision,recall)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#del model\n",
    "\n",
    "#backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
